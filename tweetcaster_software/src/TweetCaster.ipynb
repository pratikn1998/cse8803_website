{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetCaster",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr6Ko0x6MNiR"
      },
      "source": [
        "# Welcome to the Demo!\n",
        "\n",
        "Hello there! Welcome to the demo of TweetCaster! This section will guide you on how to use our project and see it in action on a small dataset (don't worry, we have done most of the dataset heavylifting for you). But before we move forward with that, we would highly recommend you to open this notebook in Google Colab. There are certain features (like downloading datasets) that might work only on that platform. Having said this, let's begin!\n",
        "\n",
        "## Step 1: Download the dependencies\n",
        "There are a set of libraries that might need to be downloaded and installed and a bunch of other libraries that you might just need to import. To do this, just run the two cells we have provided in the Project Setup section. They will automatically install everything for you while you just sit back enjoy the satisfying scene of a moving progress bar. :)\n",
        "\n",
        "## Step 2: Pre-process the dataset\n",
        "The dataset will need to be pre-processed before we can show you the magic of TweetCaster. This will hydrate the dataset and also get rid of any not-so-useful hashtags or retweet tags that Twitter occassionally adds in the tweet body. Just run the cells in dataset preparation starting from top going to the bottom one by one and you should be on your way to trying out our very first experiment. This section also has a helper that we use to show you pretty charts in our notebook.\n",
        "\n",
        "## Step 3: Tweet Sentiment Analysis\n",
        "You would need to run sentiment analysis on the tweets loaded in the dataset. This is a crucial datapoint for our project and MUST NOT be skipped for error-free run. Execute all cells in Tweet Sentiment Analysis section starting from top going all the way to the bottom one by one. Getting the subjectivity and polarity, in our experience, takes about 2 - 3 minutes. Please be patient while it runs - coffee break! Once everything is done executing, you might see a couple of graphs pop up. One of them tells you the sentiment distribution in our dataset and the other one will show you the prominent words through a word cloud.\n",
        "\n",
        "## Step 4: Adding Step 3 Results to Dataset\n",
        "Alright, this is the last boring section, I promise! All you need to do is execute the cells in the given order in the Dataset Pre-processing after Sentiment Analysis section.\n",
        "\n",
        "## Step 5: Experimentation, Evaluation, and a Cup of Tea (OPTIONAL)\n",
        "This section just shows all the approaches we tried and displays the RMSE returned by each. This step is completely optional and MAY BE SKIPPED if you would like to go directly to our main approach.\n",
        "\n",
        "### Step 5a: Regression using Sentiment Analysis (OPTIONAL)\n",
        "We have set up this section to run linear regression on the input and display the graph of predictions and true case numbers. At the end of the section, the root mean squared error (RMSE) value is also printed. Be sure to execute all cells in this section in the given order to see the expected results.\n",
        "\n",
        "### Step 5b: Time Series Models (OPTIONAL)\n",
        "We implemented some time series models for you to check out. As of now, we have ARIMA, OLS, an ensemble of the two methods and an ensemble of the two with the linear regression model from Step 5a loaded in. You may start from the top and work your way to the bottom. Each subsection will display the prediction and true values graph and print out the RMSE value.\n",
        "\n",
        "### Step 5c: Neural Network with Tweet Metadata and Sentiment Analysis (OPTIONAL)\n",
        "We implemented the neural network we suggest in our report with just the tweet metadata and sentiment analysis data. We have already pre-loaded all the hyper-parameters for you so that you can just hit the execute button on all the cells in this section and watch everything run.\n",
        "\n",
        "## Step 6: Tweet Vectorization (REQUIRED)\n",
        "The next section involves tweet vectorization. Simply run all the cells in Tweet Vectorization section and you should be good to go.\n",
        "<b>Important:</b> If you would like to use the TF-IDF embeddings, be sure to set the TF-IDF `MAX_FEATURES` variable to the maximum number of features you would like to see. We have tested it with 200 and 500 features.\n",
        "\n",
        "### Step 5d: Neural Netowrk with Tweet Encoding Only (OPTIONAL)\n",
        "While playing around with the data, we implemented the neural network on the tweet encoding data as well. This section does not assume access to the tweet metadata or its sentiment value. Feel free to run this section entirely if you would like to see how well does the model perform without rest of the data we have gathered so far. Apart from the usual data, you may also see a smoothed graph. This graph is just a 5-days moving average of the graph you were seeing earlier.\n",
        "\n",
        "## Step 7: TweetCaster - Neural Network with Tweet Encoding, Metadata and Sentiment Values (MAIN APPROACH)\n",
        "This is the main approach of our work. You may play around with the embedding we use for this part. Please read the note in the section to see exactly how to change between embedding. As of now, we have it set to execute everything using BERTweet embedding. We currently have it set to also train four models (with different sizes of the rolling window), but the code block with show just one set of graphs (for the rolling window of 7 days, since it performs the best). Change the `qualifying_entry` to (`rolling_window_length`, 7) where `rolling_window_length` can take values of 7, 14, 21 and 28 will let you see the other graphs as well. Be sure to execute all the code blocks in this section in the given order to avoid any errors.\n",
        "\n",
        "## Thank You for Checking Out TweetCaster!\n",
        "Alright, that was all! Thanks for reading the demo, and I hope that you enjoyed checking out our project. Have a great day and Happy Holidays! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9o5jOmcZJbY"
      },
      "source": [
        "# Project Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub_9AQFIzx9Y"
      },
      "source": [
        "This section mostly deals with setting up the project. This includes installing the dependencies and downloading the relevant datasets for analysis over the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQmi4PThDEu"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import itertools\n",
        "from math import sqrt\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as pyplot\n",
        "from torch import nn\n",
        "\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import drive, files\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!pip install transformers\n",
        "!pip install emoji\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "!pip install statsmodels\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtWAWG8ichoH"
      },
      "source": [
        "# Downloading raw data\n",
        "# 1. Hydrated USA Tweets Dataset\n",
        "!gdown --id 1YVnxh3rnmNb-7AWSoWpVxe24ms29nA3B\n",
        "\n",
        "# 2. COVID-19 Cases Dataset\n",
        "!wget \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snPvh8k_iWYr"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD2mGFdZ0EEJ"
      },
      "source": [
        "Once the dataset has been been downloaded, we need to preprocess it before it can be used by our approach. This preprocessing includes, hydrating the dataset (we pre-hydrated it, so we load it from a tsv file), cleaning up the tweets by getting rid of hashtags and retweets symbols, adding metadata to the tweet dataframe and calculating the number of new COVID-19 cases for the dataset retrieved from New York Times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMzSkxFviBRo"
      },
      "source": [
        "# Reading CSVs into dataframes\n",
        "covid_df = pd.read_csv('us.csv')\n",
        "usa_2021_df = pd.read_csv('2021_usa_dataset_hydrated.tsv',sep=\"\\t\", engine='python')\n",
        "\n",
        "# Aggregated tweets to be used by BERTweet\n",
        "usa_2021_agg_df = usa_2021_df.groupby('date')['text'].agg(','.join).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TDMhdC1KCX1"
      },
      "source": [
        "# Preparing the Twitter dataset\n",
        "def cleanTweets(tweet):\n",
        "   # removing mentions\n",
        "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', str(tweet))\n",
        "    # removing hashtags\n",
        "    tweet = re.sub(r'#', '', str(tweet))\n",
        "    # removing 'rt' or 'RT' from tweets\n",
        "    tweet = re.sub(r'RT : ', '', str(tweet))\n",
        "    # removing any tweets that include a link\n",
        "    tweet = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', str(tweet))\n",
        "    return tweet\n",
        "# cleaning tweets\n",
        "usa_2021_df['text'] = usa_2021_df['text'].apply(cleanTweets)\n",
        "# adding two more features\n",
        "usa_2021_df['day_of_week'] = pd.to_datetime(usa_2021_df['date']).dt.dayofweek\n",
        "sunday_mask = usa_2021_df['day_of_week'] == 0\n",
        "usa_2021_df['day_of_week'][sunday_mask] = 7\n",
        "usa_2021_df['hour_of_day'] = pd.to_datetime(usa_2021_df['time']).dt.hour\n",
        "usa_2021_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7wU1mPM2UjO"
      },
      "source": [
        "usa_2021_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0rOmLoUk4RQ"
      },
      "source": [
        "1. Adding a new column with the number of new cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71mqhcrDidkP"
      },
      "source": [
        "covid_df['new_cases'] = covid_df['cases'].diff()\n",
        "covid_df['new_cases'][0] = covid_df['cases'][0]\n",
        "\n",
        "covid_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEQLKR1Tk_rd"
      },
      "source": [
        "2. Getting data between March 01, 2021 and August 01, 2021."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWCCD05_jsYe"
      },
      "source": [
        "covid_df['date'] = pd.to_datetime(covid_df['date'])\n",
        "mask = (covid_df['date'] >= '2021-03-01') & (covid_df['date'] <= '2021-08-22')\n",
        "covid_df = covid_df[mask]\n",
        "\n",
        "covid_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObLZCgWokt4c"
      },
      "source": [
        "Visualizer code for predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf6OFEjBktJv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_model_performance(y_pred, y_target, y_label=\"Predicted Case Numbers\"):\n",
        "    # print(y_pred)\n",
        "    plt.plot(covid_df.date[-y_pred.shape[0]:], y_pred.detach().numpy(), label=y_label)\n",
        "    print(covid_df.date[-y_pred.shape[0]:].shape)\n",
        "    plt.plot(covid_df.date[-y_pred.shape[0]:], y_target.detach().numpy(), label=\"Actual Case Numbers\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHGxTGFooBKt"
      },
      "source": [
        "# Tweets Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otXINIhM3MN7"
      },
      "source": [
        "Once the dataset is ready to be analyzed, we run sentiment analysis on all tweets to check for any relations between the general sentiment from the general public and the number of new COVID-19 cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEOVTShtmz75"
      },
      "source": [
        "def getSentSubjectivity(tweet):\n",
        "    return TextBlob(tweet).sentiment.subjectivity\n",
        "\n",
        "def getSentPolarity(tweet):\n",
        "    return TextBlob(tweet).sentiment.polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwqLCdWim9Io"
      },
      "source": [
        "usa_2021_df['subjectivity'] = usa_2021_df['text'].apply(getSentSubjectivity)\n",
        "usa_2021_df['polarity'] = usa_2021_df['text'].apply(getSentPolarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_KqjkcnSn-"
      },
      "source": [
        "# negative, nautral, positive analysis\n",
        "def assignSentiment(subScore):\n",
        "    if subScore < 0:\n",
        "        return \"Negative\"\n",
        "    elif subScore == 0:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Positive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu7MkAOhnX7Z"
      },
      "source": [
        "usa_2021_df['sentiment'] = usa_2021_df['polarity'].apply(assignSentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ld01hUMoRqi"
      },
      "source": [
        "pos_tweets = usa_2021_df[usa_2021_df['sentiment'] == 'Positive']\n",
        "percent_pos_tweets = pos_tweets.shape[0]/usa_2021_df.shape[0] * 100\n",
        "print(\"The percentage of positive tweets of the whole dataset is \" + str(percent_pos_tweets) + \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRMHLrVbNqk1"
      },
      "source": [
        "num_tweets = usa_2021_df['sentiment'].value_counts().plot.bar(title='Number of Sentiment Tweets', x='sentiment', y='num_tweets')\n",
        "num_tweets.set_xlabel(\"Sentiment\")\n",
        "num_tweets.set_ylabel(\"Number of Tweets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ptHHntOfR8"
      },
      "source": [
        "usa_2021_df['date'] = pd.to_datetime(usa_2021_df['date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwIf7zMvPHHr"
      },
      "source": [
        "months_pos_list = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August']\n",
        "months_pos_nums = []\n",
        "months_neg_nums = []\n",
        "months_net_nums = []\n",
        "\n",
        "for i in range(3, 9):\n",
        "  temp_df = usa_2021_df[pd.to_datetime(usa_2021_df['date']).dt.month == i]\n",
        "  months_pos_nums.append(temp_df['sentiment'].value_counts()['Positive'])\n",
        "\n",
        "month_num_pos_sents = {\n",
        "    'month': months_pos_list[2:],\n",
        "    'Number of Positive Tweets': months_pos_nums,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QodovpiaZL06"
      },
      "source": [
        "months_pos_sent = pd.DataFrame.from_dict(month_num_pos_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ybG9PH5aAgt"
      },
      "source": [
        "month_pos_graph = months_pos_sent.plot.line(title=\"Number of Positive Tweets Jan-Aug\")\n",
        "plt.xticks(months_pos_sent.index,months_pos_sent[\"month\"].values)\n",
        "num_tweets.set_xlabel(\"Sentiment\")\n",
        "num_tweets.set_ylabel(\"Number of Tweets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMVWQZLemIka"
      },
      "source": [
        "# Creating a word cloud\n",
        "words = ' '.join([tweet for tweet in usa_2021_df['text']])\n",
        "wordCloud = WordCloud(width=600, height=400).generate(words)\n",
        "\n",
        "plt.imshow(wordCloud)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiX3TAlRYLOL"
      },
      "source": [
        "# Dataset Pre-processing after Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4crhJ4Rl1c-"
      },
      "source": [
        "3. Adding Sentiment Data to the vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TPTc9P5l0yC"
      },
      "source": [
        "date_mask = (usa_2021_df['date'] == '2021-03-01')\n",
        "print(f\"Number of tweets on March 01, 2021 is {date_mask.sum()}.\")\n",
        "positive_date_mask = date_mask & (usa_2021_df['sentiment'] == 'Positive')\n",
        "print(f\"Number of positive tweets on March 01, 2021 is {positive_date_mask.sum()}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLQmcnEYoM-5"
      },
      "source": [
        "num_positive = []\n",
        "num_negative = []\n",
        "num_neutral = []\n",
        "bucket_size = []\n",
        "hourly = [[] for _ in range(24)]\n",
        "weekday = []\n",
        "\n",
        "for curr_date in tqdm(covid_df['date']):\n",
        "    date_mask = (usa_2021_df['date'] == curr_date)\n",
        "    bucket_size.append(date_mask.sum())\n",
        "    # print(f\"Number of tweets on {curr_date} is {date_mask.sum()}.\")\n",
        "    positive_date_mask = date_mask & (usa_2021_df['sentiment'] == 'Positive')\n",
        "    negative_date_mask = date_mask & (usa_2021_df['sentiment'] == 'Negative')\n",
        "    neutral_date_mask = date_mask & (usa_2021_df['sentiment'] == 'Neutral')\n",
        "    # print(f\"Number of positive tweets on {curr_date} is {positive_date_mask.sum()}.\")\n",
        "    num_positive.append(positive_date_mask.sum())\n",
        "    num_negative.append(negative_date_mask.sum())\n",
        "    num_neutral.append(neutral_date_mask.sum())\n",
        "    for curr_hour in range(24):\n",
        "        curr_hour_mask = date_mask & (usa_2021_df['hour_of_day'] == curr_hour)\n",
        "        hourly[curr_hour].append(curr_hour_mask.sum())\n",
        "    weekday.append(curr_date.dayofweek if curr_date.dayofweek != 0 else 7)\n",
        "\n",
        "covid_df['num_positive'] = num_positive\n",
        "covid_df['num_negative'] = num_negative\n",
        "covid_df['num_neutral'] = num_neutral\n",
        "covid_df['day_of_week'] = weekday\n",
        "for curr_hour in range(24):\n",
        "    covid_df[f\"hour_{curr_hour}\"] = hourly[curr_hour]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5OhZfcnp6_Y"
      },
      "source": [
        "covid_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_H6Th8WbmsY"
      },
      "source": [
        "relevant_columns = ['num_positive', 'num_negative', 'num_neutral', 'day_of_week'] + [f'hour_{i}' for i in range(24)]\n",
        "x_data = torch.tensor(covid_df[relevant_columns].values, dtype=torch.float).t()\n",
        "y_data = torch.tensor(covid_df['new_cases'].values, dtype=float).t()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ITLb5lyw1A"
      },
      "source": [
        "x_data_np = x_data.numpy()\n",
        "y_data_np = y_data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tV-GslZgDBQ"
      },
      "source": [
        "# Regression using Sentiment Analysis\n",
        "\n",
        "We aggregate sentiment analysis data to contain just numbers of positive, negative and neutral tweets on each day. We then perform linear regression to predict the number of infections on that day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvHUH2o7gGaz"
      },
      "source": [
        "relevant_columns = ['num_positive', 'num_negative', 'num_neutral']\n",
        "\n",
        "num_cols = len(relevant_columns)  # Postive, Negative, Neutral\n",
        "\n",
        "A = torch.randn((1, num_cols), requires_grad=True)\n",
        "b = torch.randn((1, 1), requires_grad=True)\n",
        "\n",
        "def layer_1(X):\n",
        "    # print(X.shape)\n",
        "    # print(A.shape)\n",
        "    return A.mm(X) + b\n",
        "\n",
        "def mlr_model(X):\n",
        "    return layer_1(X)\n",
        "\n",
        "def loss(y_predicted, y_target):\n",
        "    return torch.sqrt(torch.mean((y_predicted - y_target) ** 2))\n",
        "\n",
        "optimizer = torch.optim.Adam([A, b], lr=0.1)\n",
        "num_epochs = 1000\n",
        "\n",
        "curr_x = torch.tensor(covid_df[relevant_columns].values, dtype=torch.float).t()\n",
        "\n",
        "x_train, x_test = curr_x[:, :int(x_data.shape[1] * 0.8)], curr_x[:, int(x_data.shape[1] * 0.8):]\n",
        "y_train, y_test = y_data[:int(y_data.shape[0] * 0.8)], y_data[int(y_data.shape[0] * 0.8):]\n",
        "\n",
        "with trange(num_epochs, desc=\"Training Multi-variable Linear Regression\") as progress_bar:\n",
        "    for _ in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = mlr_model(x_train)\n",
        "        curr_loss = loss(y_pred, y_train)\n",
        "        curr_loss.backward()\n",
        "        optimizer.step()\n",
        "        progress_bar.set_postfix(loss=curr_loss.data)\n",
        "        # print(f\"Epoch: {curr_epoch}, Loss: {curr_loss}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pGL7N9yri9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "mlr_pred = mlr_model(x_test)[0, :]\n",
        "show_model_performance(mlr_pred, y_test, \"MLR Predicted\")\n",
        "# mlr_pred = mlr_model(x_test)\n",
        "mlr_rmse = loss(mlr_pred, y_test)\n",
        "# plt.plot(covid_df.date, mlr_pred[0, :].detach().numpy(), label=\"MLR Predicted - WITH hour of day\")\n",
        "# # plt.plot(covid_df.date, temp_var, label=\"MLR Predicted - NO hour of day\")\n",
        "# plt.plot(covid_df.date, y_test.detach().numpy(), label=\"Actual Case Numbers\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIebWZjuoPKG"
      },
      "source": [
        "mlr_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vfvxdc7S7sM"
      },
      "source": [
        "# Time Series Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaPRC6lp9Oc0"
      },
      "source": [
        "Since new COVID-19 cases is a time series dataset, we try to fit time series models in this section to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y5WoHQuS_LH"
      },
      "source": [
        "## ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rsa_BP9dkOh"
      },
      "source": [
        "arima_columns = ['date', 'new_cases']\n",
        "arima_df = covid_df[arima_columns]\n",
        "arima_df['date'] = pd.DatetimeIndex(arima_df['date']).month\n",
        "arima_df = arima_df.set_index('date')\n",
        "X_arima = arima_df.values\n",
        "\n",
        "size_arima = int(len(X_arima) * 0.8)\n",
        "train, test = X_arima[0:size_arima], X_arima[size_arima:len(X_arima)]\n",
        "\n",
        "history = [x for x in train]\n",
        "predictions_arima = list()\n",
        "test = list(test)\n",
        "# # walk-forward validation\n",
        "for t in range(len(test)):\n",
        "\tmodel = ARIMA(history, order=(2,0,2))\n",
        "\tmodel_fit = model.fit()\n",
        "\toutput = model_fit.forecast()\n",
        "\tyhat = output[0]\n",
        "\tpredictions_arima.append(yhat)\n",
        "\tobs = test[t]\n",
        "\thistory.append(obs)\n",
        "\t# print('predicted=%f, expected=%f' % (yhat, obs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOiSywaOdqZP"
      },
      "source": [
        "arima_rmse = sqrt(mean_squared_error(test, predictions_arima))\n",
        "print('Test RMSE: %.3f' % arima_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMjY-1gzdtH3"
      },
      "source": [
        "plt.plot(test)\n",
        "plt.plot(predictions_arima, color='red')\n",
        "legend1 = plt.legend([\"actual number of new cases\", \"ARIMA predicted new cases\"], loc=1)\n",
        "labels = ['July', 'August']\n",
        "x = [0, 30]\n",
        "plt.title('ARIMA model')\n",
        "plt.xticks(x, labels)\n",
        "plt.ylabel(\"Number of New Covid Cases\")\n",
        "plt.xlabel(\"Month in 2021\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkgoTgVKTCMB"
      },
      "source": [
        "## OLS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3vhbH_AdwBi"
      },
      "source": [
        "ols_columns = ['new_cases', 'num_positive', 'num_negative',\n",
        "       'num_neutral', 'day_of_week', 'hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
        "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
        "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
        "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
        "       'hour_23']\n",
        "ols_df = covid_df[ols_columns]\n",
        "# train_ols, test_ols = train_test_split(ols_df, test_size=0.3)\n",
        "train_ols = ols_df.head(int(0.8*ols_df.shape[0]))\n",
        "test_ols = ols_df.tail(int(0.2*ols_df.shape[0]))\n",
        "print(ols_df.shape)\n",
        "\n",
        "X_train = train_ols[['num_positive', 'num_negative',\n",
        "       'num_neutral', 'day_of_week', 'hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
        "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
        "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
        "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
        "       'hour_23']].values\n",
        "Y_train = train_ols['new_cases'].values\n",
        "\n",
        "X_test = test_ols[['num_positive', 'num_negative',\n",
        "       'num_neutral', 'day_of_week', 'hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
        "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
        "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
        "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
        "       'hour_23']].values\n",
        "y_test = test_ols['new_cases'].values\n",
        "\n",
        "X_history_ols = [x for x in X_train]\n",
        "Y_history_ols = [y for y in Y_train]\n",
        "predictions_ols = []\n",
        "for t in range(len(y_test)):\n",
        "    model = OLS(Y_history_ols, X_history_ols).fit()\n",
        "    # print(X_test[t].shape)\n",
        "    pred = model.predict(X_test[t])[0]\n",
        "    truth = y_test[t]\n",
        "    predictions_ols.append(pred)\n",
        "    X_history_ols.append(X_test[t])\n",
        "    Y_history_ols.append(pred)\n",
        "ols_rmse = sqrt(mean_squared_error(y_test, predictions_ols))\n",
        "print('Test RMSE: %.3f' % ols_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS6W4uPidxrR"
      },
      "source": [
        "# show_model_performance(torch.tensor(predictions_ols), torch.tensor(y_test), \"OLS Predictions\")\n",
        "plt.plot(y_test)\n",
        "plt.plot(predictions_ols, color='red')\n",
        "legend1 = plt.legend([\"actual number of new cases\", \"OLS predicted new cases\"], loc=1)\n",
        "labels = ['July', 'August']\n",
        "x = [0, 30]\n",
        "plt.xticks(x, labels)\n",
        "plt.ylabel(\"Number of New Covid Cases\")\n",
        "plt.xlabel(\"Month in 2021\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ODTgDBYSH7K"
      },
      "source": [
        "## Ensemble Method - ARIMA, OLS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnpO7REtdz9X"
      },
      "source": [
        "X_history_em2 = [x for x in X_train]\n",
        "Y_history_em2 = [y for y in Y_train]\n",
        "\n",
        "model_ARIMA = ARIMA(Y_history_em2, order=(2, 0, 2)).fit()\n",
        "pred_train_ARIMA = model_ARIMA.predict()\n",
        "rmse_train_ARIMA = sqrt(mean_squared_error(Y_history_em2, pred_train_ARIMA))\n",
        "\n",
        "model_OLS = OLS(Y_history_em2, X_history_em2).fit()\n",
        "pred_train_OLS = model_OLS.predict(X_history_em2)\n",
        "rmse_train_OLS = sqrt(mean_squared_error(Y_history_em2, pred_train_OLS))\n",
        "\n",
        "w1 = 1 / rmse_train_ARIMA\n",
        "w2 = 1 / rmse_train_OLS\n",
        "\n",
        "em2_predictions = []\n",
        "for t in range(len(y_test)):\n",
        "    model_OLS = OLS(Y_history_em2, X_history_em2).fit()\n",
        "    pred_OLS = model_OLS.predict(X_test[t])[0]\n",
        "\n",
        "    model_ARIMA = ARIMA(Y_history_em2, order=(2, 0, 2)).fit()\n",
        "    pred_ARIMA = model_ARIMA.forecast()[0]\n",
        "\n",
        "    pred = (w1 * pred_ARIMA + w2 * pred_OLS) / (w1 + w2)\n",
        "    em2_predictions.append(pred)\n",
        "    X_history_em2.append(X_test[t])\n",
        "    Y_history_em2.append(pred)\n",
        "\n",
        "rmse_em2 = sqrt(mean_squared_error(y_test, em2_predictions))\n",
        "print('Test RMSE: %.3f' % rmse_em2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9bHaZ5nd1sL"
      },
      "source": [
        "plt.plot(y_test)\n",
        "plt.plot(em2_predictions, color='red')\n",
        "legend1 = plt.legend([\"actual number of new cases\", \"predicted ARIMA + OLS new cases\"], loc=1)\n",
        "labels = ['July', 'August']\n",
        "x = [0, 30]\n",
        "plt.xticks(x, labels)\n",
        "plt.ylabel(\"Number of New Covid Cases\")\n",
        "plt.xlabel(\"Month in 2021\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Zx0zV06L92"
      },
      "source": [
        "## Ensemble Method - Linear Regression with Time Series Models\n",
        "\n",
        "We run another experiment to check how would the time series models perform with the previously-tried linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqOV66R-6Qq9"
      },
      "source": [
        "arima_columns = ['date', 'new_cases']\n",
        "arima_df = covid_df[arima_columns]\n",
        "arima_df['date'] = pd.DatetimeIndex(arima_df['date']).month\n",
        "arima_df = arima_df.set_index('date')\n",
        "X_arima = arima_df.values\n",
        "\n",
        "size_arima = int(len(X_arima) * 0.7)\n",
        "arima_train, test = X_arima[0:size_arima], X_arima[size_arima:len(X_arima)]\n",
        "X_history_ols, X_test = x_data[: size_arima], x_data[size_arima:]\n",
        "\n",
        "curr_x = torch.tensor(covid_df[relevant_columns].values, dtype=torch.float).t()\n",
        "\n",
        "data_received = list(curr_x[:, :size_arima].numpy().T)\n",
        "data_hidden = curr_x[:, size_arima:].numpy().T\n",
        "\n",
        "history = list(Y_train[:size_arima])\n",
        "test = list(test)\n",
        "\n",
        "predictions = []\n",
        "# # walk-forward validation\n",
        "for t in range(len(test)):\n",
        "\tmodel = ARIMA(history, order=(2,0,2))\n",
        "\tmodel_fit = model.fit()\n",
        "\tarima_pred = model_fit.forecast()[0][0]\n",
        "\n",
        "\tmodel = OLS(history, data_received).fit()\n",
        "\t# print(data_hidden[t].shape)\n",
        "\tols_pred = model.predict(data_hidden[t])[0]\n",
        "\tif isinstance(ols_pred, np.ndarray):\n",
        "\t\tols_pred = ols_pred[0]\n",
        "\n",
        "\tmlr_pred = mlr_model(torch.Tensor([data_hidden[t]]).t())\n",
        "\t\n",
        "\t# print((1 / arima_rmse) * arima_pred)\n",
        "\t# print(ols_pred)\n",
        "\t\n",
        "\tfinal_pred = ((1 / arima_rmse) * arima_pred + (1 / ols_rmse) * ols_pred + (1 / mlr_rmse) * mlr_pred) / (1 / arima_rmse + 1 / ols_rmse + 1 / mlr_rmse)\n",
        "\t# final_pred = ((1 / ols_rmse) * ols_pred + (1 / mlr_rmse) * mlr_pred) / (1 / ols_rmse + 1 / mlr_rmse)\n",
        "\n",
        "\tobs = test[t]\n",
        "\thistory.append(obs)\n",
        "\tdata_received.append(data_hidden[t])\n",
        " \n",
        "\tpredictions.append(final_pred)\n",
        "\t# print('predicted=%f, expected=%f' % (final_pred, obs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdi9rzupMuPr"
      },
      "source": [
        "# print(len(predictions))\n",
        "# print(len(test))\n",
        "show_model_performance(torch.tensor(predictions), torch.tensor(test), \"Ensemble Predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LLmUm-cKcHn"
      },
      "source": [
        "ensemble_rmse = loss(torch.tensor(predictions), torch.tensor(test))\n",
        "ensemble_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js-T_3ps85dp"
      },
      "source": [
        "# Neural Network with Tweet Metadata and Sentiment Analysis Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dck9YufG1qOK"
      },
      "source": [
        "relevant_columns = ['num_positive', 'num_negative', 'num_neutral', 'day_of_week'] + [f'hour_{i}' for i in range(24)]\n",
        "\n",
        "num_cols = len(relevant_columns)  # Postive, Negative, Neutral\n",
        "\n",
        "A = torch.randn((20, num_cols), requires_grad=True)\n",
        "b = torch.randn((20, 1), requires_grad=True)\n",
        "C = torch.randn((1, 20), requires_grad=True)\n",
        "d = torch.randn(1, requires_grad=True)\n",
        "\n",
        "max_num = 0\n",
        "\n",
        "def layer_1(X):\n",
        "    # print(X.shape)\n",
        "    # print(A.shape)\n",
        "    return A.mm(X) + b\n",
        "\n",
        "def layer_2(X):\n",
        "    return C.mm(X) + d\n",
        "\n",
        "def mlr_model(X):\n",
        "    return layer_2(layer_1(X))\n",
        "\n",
        "def loss(y_predicted, y_target):\n",
        "    return torch.sqrt(torch.mean((y_predicted - y_target) ** 2))\n",
        "\n",
        "def predict(X):\n",
        "    initial_prediction = layer_2(layer_1(X))\n",
        "    initial_prediction[initial_prediction < 0] = 0\n",
        "    initial_prediction[initial_prediction > max_num + 50000] = max_num + 50000\n",
        "    return initial_prediction\n",
        "\n",
        "def reset_model():\n",
        "    A = torch.randn((20, num_cols), requires_grad=True)\n",
        "    b = torch.randn((20, 1), requires_grad=True)\n",
        "    C = torch.randn((1, 20), requires_grad=True)\n",
        "    d = torch.randn(1, requires_grad=True)\n",
        "    max_num = 0\n",
        "\n",
        "optimizer = torch.optim.Adam([A, b], lr=0.1)\n",
        "num_epochs = 10\n",
        "rolling_options = [7]\n",
        "rolling_window_length = 21\n",
        "forecast_options = [7]\n",
        "forecast_window = 7\n",
        "\n",
        "x_train, x_test = x_data[:, :int(x_data.shape[1] * 0.8)], x_data[:, int(x_data.shape[1] * 0.8):]\n",
        "y_train, y_test = y_data[:int(y_data.shape[0] * 0.8)], y_data[int(y_data.shape[0] * 0.8):]\n",
        "\n",
        "curr_loss = 0\n",
        "num_items = 0\n",
        "\n",
        "prediction_tracker = {}\n",
        "target_y_tracker = {}\n",
        "loss_tracker = {}\n",
        "\n",
        "for rolling_window_length, forecast_window in list(itertools.product(rolling_options, forecast_options)):\n",
        "    target_y = []\n",
        "    predictions = []\n",
        "    with trange(x_data.shape[1] - rolling_window_length - 2 * forecast_window - 1, desc=\"Training 21-day rolling MLR\") as main_bar:\n",
        "        # main_bar.set_postfix(rolling_window_length=rolling_window_length, forecast_window=forecast_window)\n",
        "        for start_idx in main_bar:\n",
        "            reset_model()\n",
        "            curr_x = x_data[:, start_idx: start_idx + rolling_window_length]\n",
        "            curr_y = y_data[start_idx + forecast_window: start_idx + forecast_window + rolling_window_length]\n",
        "            # with trange(num_epochs, desc=\"Training Multi-variable Linear Regression\") as progress_bar:\n",
        "                # for _ in progress_bar:\n",
        "            for _ in range(num_epochs):\n",
        "                max_num = max(curr_y)\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = mlr_model(curr_x)\n",
        "                curr_loss = loss(y_pred, curr_y)\n",
        "                curr_loss.backward()\n",
        "                optimizer.step()\n",
        "                # progress_bar.set_postfix(loss=curr_loss.data)\n",
        "                # print(f\"Epoch: {curr_epoch}, Loss: {curr_loss}\")\n",
        "            curr_pred = predict(x_data[:, start_idx + rolling_window_length + forecast_window: start_idx + rolling_window_length + forecast_window + 1])\n",
        "            predictions.append(curr_pred[0])\n",
        "            target_y.append(y_data[start_idx + 2 * forecast_window + rolling_window_length: start_idx + 2 * forecast_window + rolling_window_length + 1])\n",
        "            overall_loss = loss(torch.tensor(predictions), torch.tensor(target_y))\n",
        "            main_bar.set_postfix(loss=overall_loss, rolling_window_length=rolling_window_length, forecast_window=forecast_window)\n",
        "    prediction_tracker[(rolling_window_length, forecast_window)] = predictions\n",
        "    target_y_tracker[(rolling_window_length, forecast_window)] = target_y\n",
        "    loss_tracker[(rolling_window_length, forecast_window)] = overall_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT7uskQ36zl_"
      },
      "source": [
        "qualifying_entry = (7, 7)\n",
        "predictions = prediction_tracker[qualifying_entry]\n",
        "target_y = target_y_tracker[qualifying_entry]\n",
        "\n",
        "show_model_performance(torch.tensor(predictions), torch.tensor(target_y), \"TweetCaster Predicted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4sKVLoYN75v"
      },
      "source": [
        "rolling_mean = covid_df['new_cases'].rolling(window=14).mean()\n",
        "pred_df = pd.DataFrame(predictions).rolling(window=7).mean()\n",
        "target_df = pd.DataFrame(target_y).rolling(window=7).mean()\n",
        "pred_moving_avg_values = torch.tensor(pred_df.values)\n",
        "target_moving_avg_values = torch.tensor(target_df.values)\n",
        "show_model_performance(pred_moving_avg_values, target_moving_avg_values, \"TweetCaster Predicted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZZ5jgDeZ2Y-"
      },
      "source": [
        "# Tweet Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37ULi1brRRgN"
      },
      "source": [
        "This section covers experiments we run with tweet vectorization. The general idea is to check how does the information about the tweets themselves along with their metadata affect the predictions generated.\n",
        "\n",
        "This section also tests how do the different types of encodings affect the kind of results produced. The ones we test in our work include,\n",
        "\n",
        "1. TF-IDF with 200 features\n",
        "2. TF-IDF with 500 features\n",
        "3. BERTweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EaDIDkzSGVx"
      },
      "source": [
        "Note: You may change the MAX_FEATURES variable to change the number of features that TF-IDF encoding generates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFJH1SW0aJbb"
      },
      "source": [
        "# Parameter for TF-IDF vectorization\n",
        "MAX_FEATURES = 200\n",
        "# 1. TF-IDF vectorization using TweetTokenizer (ABANDONED)\n",
        "def tfidf_vectorization_tweet_tokenizer(df, max_features):\n",
        "    tt = TweetTokenizer(preserve_case=False)\n",
        "    vector = TfidfVectorizer(\n",
        "        sublinear_tf=True,\n",
        "        max_features=max_features,\n",
        "        token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b',\n",
        "        tokenizer=tt.tokenize\n",
        "    )\n",
        "    X = vector.fit_transform(df['text'].values)\n",
        "    X_col = vector.get_feature_names()\n",
        "    feature_df = pd.DataFrame.sparse.from_spmatrix(X, columns=X_col)\n",
        "    # feature_df['tweet_id'] = df['tweet_id']\n",
        "    return feature_df\n",
        "\n",
        "# Vectorized tweet using TF-IDF\n",
        "vectorized_df = tfidf_vectorization_tweet_tokenizer(usa_2021_agg_df, MAX_FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxnLvxpdK3b"
      },
      "source": [
        "# 2. BERT\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "def bert_vectorization(df, pre_trained_model):\n",
        "    feature_df = pd.DataFrame()\n",
        "    # feature_df['day_of_week'] = df['day_of_week']\n",
        "    # feature_df['hour_of_day'] = df['hour_of_day']\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pre_trained_model, normalization=True)\n",
        "    model = AutoModel.from_pretrained(pre_trained_model)\n",
        "    sentences = list(df['text'].values)\n",
        "\n",
        "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    \n",
        "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    return embeddings\n",
        "\n",
        "    # feature_df['vector'] = list(embeddings)\n",
        "    # return feature_df\n",
        "\n",
        "BERTWEET_PRE_TRAINED = 'vinai/bertweet-base'\n",
        "vectorized_emb = bert_vectorization(usa_2021_agg_df, BERTWEET_PRE_TRAINED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quos28cyFeCg"
      },
      "source": [
        "# Neural Network with Tweet Encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ32UPJqzhLv"
      },
      "source": [
        "num_cols = 768  # Postive, Negative, Neutral\n",
        "\n",
        "A = torch.randn((20, num_cols), requires_grad=True)\n",
        "b = torch.randn((20, 1), requires_grad=True)\n",
        "C = torch.randn((1, 20), requires_grad=True)\n",
        "d = torch.randn(1, requires_grad=True)\n",
        "\n",
        "def layer_1(X):\n",
        "    # print(A.shape)\n",
        "    # print(X.shape)\n",
        "    # print(b.shape)\n",
        "    return A.mm(X) + b\n",
        "\n",
        "def layer_2(X):\n",
        "    return C.mm(X) + d\n",
        "\n",
        "def mlr_model(X):\n",
        "    return layer_2(layer_1(X))\n",
        "\n",
        "def loss(y_predicted, y_target):\n",
        "    return torch.sqrt(torch.mean((y_predicted - y_target) ** 2))\n",
        "\n",
        "def predict(X):\n",
        "    initial_prediction = layer_2(layer_1(X))\n",
        "    initial_prediction[initial_prediction < 0] = 0\n",
        "    initial_prediction[initial_prediction > max_num + 50000] = max_num + 50000\n",
        "    return initial_prediction\n",
        "\n",
        "def reset_model():\n",
        "    A = torch.randn((20, num_cols), requires_grad=True)\n",
        "    b = torch.randn((20, 1), requires_grad=True)\n",
        "    C = torch.randn((1, 20), requires_grad=True)\n",
        "    d = torch.randn(1, requires_grad=True)\n",
        "    max_num = 0\n",
        "\n",
        "optimizer = torch.optim.Adam([A, b], lr=0.1)\n",
        "num_epochs = 500\n",
        "rolling_options = [7]\n",
        "rolling_window_length = 21\n",
        "forecast_options = [7]\n",
        "forecast_window = 7\n",
        "\n",
        "overall_x = vectorized_emb.t()[:, -175:-7]\n",
        "overall_y = y_data.t()[7:]\n",
        "\n",
        "curr_loss = 0\n",
        "num_items = 0\n",
        "\n",
        "prediction_tracker = {}\n",
        "target_y_tracker = {}\n",
        "loss_tracker = {}\n",
        "\n",
        "for rolling_window_length, forecast_window in list(itertools.product(rolling_options, forecast_options)):\n",
        "    target_y = []\n",
        "    predictions = []\n",
        "    with trange(overall_x.shape[1] - rolling_window_length - 2 * forecast_window - 1, desc=\"Training 21-day rolling MLR\") as main_bar:\n",
        "        # main_bar.set_postfix(rolling_window_length=rolling_window_length, forecast_window=forecast_window)\n",
        "        for start_idx in main_bar:\n",
        "            reset_model()\n",
        "            curr_x = overall_x[:, start_idx: start_idx + rolling_window_length]\n",
        "            curr_y = overall_y[start_idx + forecast_window: start_idx + forecast_window + rolling_window_length]\n",
        "            # with trange(num_epochs, desc=\"Training Multi-variable Linear Regression\") as progress_bar:\n",
        "                # for _ in progress_bar:\n",
        "            for _ in range(num_epochs):\n",
        "                max_num = max(curr_y)\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = mlr_model(curr_x)\n",
        "                curr_loss = loss(y_pred, curr_y)\n",
        "                curr_loss.backward()\n",
        "                optimizer.step()\n",
        "                # progress_bar.set_postfix(loss=curr_loss.data)\n",
        "                # print(f\"Epoch: {curr_epoch}, Loss: {curr_loss}\")\n",
        "            curr_pred = predict(overall_x[:, start_idx + rolling_window_length + forecast_window: start_idx + rolling_window_length + forecast_window + 1])\n",
        "            predictions.append(curr_pred[0])\n",
        "            target_y.append(overall_y[start_idx + 2 * forecast_window + rolling_window_length: start_idx + 2 * forecast_window + rolling_window_length + 1])\n",
        "            overall_loss = loss(torch.tensor(predictions), torch.tensor(target_y))\n",
        "            main_bar.set_postfix(loss=overall_loss, rolling_window_length=rolling_window_length, forecast_window=forecast_window)\n",
        "    prediction_tracker[(rolling_window_length, forecast_window)] = predictions\n",
        "    target_y_tracker[(rolling_window_length, forecast_window)] = target_y\n",
        "    loss_tracker[(rolling_window_length, forecast_window)] = overall_loss\n",
        "\n",
        "\n",
        "\n",
        "# optimizer = torch.optim.Adam([A, b], lr=0.1)\n",
        "# num_epochs = 5000\n",
        "\n",
        "# x_train, x_test = x_data[:, :int(x_data.shape[1] * 0.8)], x_data[:, int(x_data.shape[1] * 0.8):]\n",
        "# y_train, y_test = y_data[:int(y_data.shape[0] * 0.8)], y_data[int(y_data.shape[0] * 0.8):]\n",
        "# overall_x = vectorized_emb.t()[:, -175:-7]\n",
        "# overall_y = y_data.t()[7:]\n",
        "# x_train, x_test = overall_x[:, :int(0.8 * overall_x.shape[1])], overall_x[:, int(0.8 * overall_x.shape[1]):]\n",
        "# y_train, y_test = overall_y[:int(0.8 * overall_x.shape[1])], overall_y[int(0.8 * overall_x.shape[1]):]\n",
        "\n",
        "# with trange(num_epochs, desc=\"Training Multi-variable Linear Regression\") as progress_bar:\n",
        "#     for _ in progress_bar:\n",
        "#         optimizer.zero_grad()\n",
        "#         y_pred = mlr_model(x_train)\n",
        "#         curr_loss = loss(y_pred, y_train)\n",
        "#         curr_loss.backward()\n",
        "#         optimizer.step()\n",
        "#         progress_bar.set_postfix(loss=curr_loss.data)\n",
        "#         # print(f\"Epoch: {curr_epoch}, Loss: {curr_loss}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BBtoX640pLf"
      },
      "source": [
        "qualifying_entry = (7, 7)\n",
        "predictions = prediction_tracker[qualifying_entry]\n",
        "target_y = target_y_tracker[qualifying_entry]\n",
        "\n",
        "show_model_performance(torch.tensor(predictions), torch.tensor(target_y), \"PlainTweet Predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yvqs7BS-ydp"
      },
      "source": [
        "rolling_mean = covid_df['new_cases'].rolling(window=14).mean()\n",
        "pred_df = pd.DataFrame(predictions).rolling(window=7).mean()\n",
        "target_df = pd.DataFrame(target_y).rolling(window=7).mean()\n",
        "pred_moving_avg_values = torch.tensor(pred_df.values)\n",
        "target_moving_avg_values = torch.tensor(target_df.values)\n",
        "show_model_performance(pred_moving_avg_values, target_moving_avg_values, \"PlainTweet Predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaVJOelEYn_r"
      },
      "source": [
        "# TweetCaster: Neural Network with Tweet Encodings, Metadata and Sentiment Analysis Data\n",
        "\n",
        "Currently assumes the vectorized tweets along with sentiment values as inputs. Note that the sentiment must be represented by an integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44l0is-YTx76"
      },
      "source": [
        "This section focuses on training the neural network to be able to predict the number of new COVID-19 cases seven days in the future. Please be sure to update the DIM_FEATURES before using this function. It must be set to the number of features + 28, if using TF-IDF, or 796, if using BERTweet.\n",
        "\n",
        "Also, if you use BERTweet, please comment out line 4 in the code block below and un-comment line 5. If you use TF-IDF, please comment out line 5 in the code block below and un-comment line 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtaCLMQwGgjK"
      },
      "source": [
        "DIM_FEATURES = 796\n",
        "num_cols = DIM_FEATURES\n",
        "\n",
        "# combined_x = torch.vstack((torch.from_numpy(vectorized_df.values).float()[-x_data.shape[1]:, :].t(), x_data))  # if using TF-IDF, comment if using BERTweet\n",
        "combined_x = torch.vstack((vectorized_emb.float()[-x_data.shape[1]:, :].t(), x_data))  # if using BERTweet, comment if using TF-IDF\n",
        "\n",
        "A = torch.randn((20, num_cols), requires_grad=True)\n",
        "b = torch.randn((20, 1), requires_grad=True)\n",
        "C = torch.randn((1, 20), requires_grad=True)\n",
        "d = torch.randn(1, requires_grad=True)\n",
        "\n",
        "def layer_1(X):\n",
        "    # print(A.shape)\n",
        "    # print(X.shape)\n",
        "    # print(b.shape)\n",
        "    return A.mm(X) + b\n",
        "\n",
        "def layer_2(X):\n",
        "    return C.mm(X) + d\n",
        "\n",
        "def mlr_model(X):\n",
        "    return layer_2(layer_1(X))\n",
        "\n",
        "def loss(y_predicted, y_target):\n",
        "    return torch.sqrt(torch.mean((y_predicted - y_target) ** 2))\n",
        "\n",
        "def predict(X):\n",
        "    initial_prediction = layer_2(layer_1(X))\n",
        "    initial_prediction[initial_prediction < 0] = 0\n",
        "    initial_prediction[initial_prediction > max_num + 50000] = max_num + 50000\n",
        "    return initial_prediction\n",
        "\n",
        "def reset_model():\n",
        "    A = torch.randn((20, num_cols), requires_grad=True)\n",
        "    b = torch.randn((20, 1), requires_grad=True)\n",
        "    C = torch.randn((1, 20), requires_grad=True)\n",
        "    d = torch.randn(1, requires_grad=True)\n",
        "    max_num = 0\n",
        "\n",
        "optimizer = torch.optim.Adam([A, b], lr=0.1)\n",
        "num_epochs = 10\n",
        "rolling_options = [7, 14, 21, 28]\n",
        "rolling_window_length = 21\n",
        "forecast_options = [7]\n",
        "forecast_window = 7\n",
        "\n",
        "overall_x = combined_x[:, -175:-7]\n",
        "overall_y = y_data.t()[7:]\n",
        "\n",
        "curr_loss = 0\n",
        "num_items = 0\n",
        "\n",
        "prediction_tracker = {}\n",
        "target_y_tracker = {}\n",
        "loss_tracker = {}\n",
        "\n",
        "for rolling_window_length, forecast_window in list(itertools.product(rolling_options, forecast_options)):\n",
        "    target_y = []\n",
        "    predictions = []\n",
        "    with trange(overall_x.shape[1] - rolling_window_length - 2 * forecast_window - 1, desc=\"Training variable-day rolling BERTweet Neural Network\") as main_bar:\n",
        "        # main_bar.set_postfix(rolling_window_length=rolling_window_length, forecast_window=forecast_window)\n",
        "        for start_idx in main_bar:\n",
        "            reset_model()\n",
        "            curr_x = overall_x[:, start_idx: start_idx + rolling_window_length]\n",
        "            curr_y = overall_y[start_idx + forecast_window: start_idx + forecast_window + rolling_window_length]\n",
        "            # with trange(num_epochs, desc=\"Training Multi-variable Linear Regression\") as progress_bar:\n",
        "                # for _ in progress_bar:\n",
        "            for _ in range(num_epochs):\n",
        "                max_num = max(curr_y)\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = mlr_model(curr_x)\n",
        "                curr_loss = loss(y_pred, curr_y)\n",
        "                curr_loss.backward()\n",
        "                optimizer.step()\n",
        "                # progress_bar.set_postfix(loss=curr_loss.data)\n",
        "                # print(f\"Epoch: {curr_epoch}, Loss: {curr_loss}\")\n",
        "            curr_pred = predict(overall_x[:, start_idx + rolling_window_length + forecast_window: start_idx + rolling_window_length + forecast_window + 1])\n",
        "            predictions.append(curr_pred[0])\n",
        "            target_y.append(overall_y[start_idx + 2 * forecast_window + rolling_window_length: start_idx + 2 * forecast_window + rolling_window_length + 1])\n",
        "            overall_loss = loss(torch.tensor(predictions), torch.tensor(target_y))\n",
        "            main_bar.set_postfix(loss=overall_loss, rolling_window_length=rolling_window_length, forecast_window=forecast_window)\n",
        "    prediction_tracker[(rolling_window_length, forecast_window)] = predictions\n",
        "    target_y_tracker[(rolling_window_length, forecast_window)] = target_y\n",
        "    loss_tracker[(rolling_window_length, forecast_window)] = overall_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM-b7pkLJaVa"
      },
      "source": [
        "qualifying_entry = (7, 7)\n",
        "predictions = prediction_tracker[qualifying_entry]\n",
        "target_y = target_y_tracker[qualifying_entry]\n",
        "\n",
        "show_model_performance(torch.tensor(predictions), torch.tensor(target_y), \"TweetCaster Predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W1dll94JcpX"
      },
      "source": [
        "rolling_mean = covid_df['new_cases'].rolling(window=14).mean()\n",
        "pred_df = pd.DataFrame(predictions).rolling(window=7).mean()\n",
        "target_df = pd.DataFrame(target_y).rolling(window=7).mean()\n",
        "pred_moving_avg_values = torch.tensor(pred_df.values)\n",
        "target_moving_avg_values = torch.tensor(target_df.values)\n",
        "show_model_performance(pred_moving_avg_values, target_moving_avg_values, \"TweetCaster Predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}